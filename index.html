<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="description" content="Alohomora Labs: Magical research in motion, gait analysis, and biotech. Home of GaitSetPy." />
  <title>Alohomora Labs</title>
  <link rel="icon" href="favicon.ico" type="image/x-icon" />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&family=Marcellus&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header class="site-header">
    <div class="container nav-flex">
      <div class="logo">Alohomora Labs</div>
      <button class="hamburger" onclick="toggleMenu()">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <nav id="nav-menu">
        <a href="index.html" class="active">Home</a>
        <a href="about.html">About</a>
        <a href="team.html">Team</a>
        <a href="gaitsetpy.html">GaitSetPy</a>
        <a href="https://halo.alohomora-labs.me" target="_blank">Halo</a>
      </nav>
    </div>
  </header>
  <main>
    <section class="hero">
      <div class="container">
        <h1>Unlocking the Magic of Motion</h1>
        <p class="subtitle">A research lab exploring the frontiers of gait, motion, and biotech with machine learning and deep learning.</p>
      </div>
    </section>
    <section class="highlights">
      <div class="container highlights-flex">
        <div class="highlight-card">
          <h2>What We Do</h2>
          <p>We blend science and creativity to advance motion analysis, gait research, and biotechnology. Our work bridges data, algorithms, and the wonders of human movement.</p>
        </div>
        <div class="highlight-card">
          <h2>Featured Toolkit</h2>
          <p><strong>GaitSetPy</strong> — a professional, extensible toolkit for gait analysis, available on PyPI. Empowering researchers worldwide.</p>
          <a href="gaitsetpy.html" class="button">Learn More</a>
        </div>
        <div class="highlight-card">
          <h2>Our Team</h2>
          <p>Driven by curiosity and expertise, our team brings together research, engineering, and a spark of magic. Meet the minds behind the lab.</p>
          <a href="team.html" class="button">Meet the Team</a>
        </div>
      </div>
    </section>
    <section class="halo-talks">
      <div class="container">
        <h2 class="halo-title">Halo Lab Talks</h2>
        <div class="halo-content">
          <p>Join us for <strong>Halo</strong> — our series of lab talks where we share insights, discuss cutting-edge research, and explore the fascinating world of motion analysis and biotechnology. These sessions bring together researchers, practitioners, and enthusiasts to dive deep into the latest developments in our field.</p>
          <div class="halo-features">
            <div class="halo-feature">
              <h3>Research Presentations</h3>
              <p>Deep dives into our latest findings in gait analysis, motion recognition, and biotech applications.</p>
            </div>
            <div class="halo-feature">
              <h3>Tool Demonstrations</h3>
              <p>Live demonstrations of GaitSetPy and other tools we're developing for the research community.</p>
            </div>
            <div class="halo-feature">
              <h3>Community Discussions</h3>
              <p>Interactive sessions where we discuss challenges, share experiences, and explore future directions.</p>
            </div>
          </div>
          <div class="halo-cta">
            <a href="https://halo.alohomora-labs.com" target="_blank" class="button">Visit Halo</a>
          </div>
        </div>
      </div>
    </section>
    <section class="publications">
      <div class="container">
        <h2 class="pubs-title">Publications & Works</h2>
        <div class="pub-list">
          <div class="pub-item">
            <div class="pub-header" style="display:flex; align-items:flex-start; justify-content:space-between; gap:1.5em;">
              <div class="pub-header-main">
                <span class="pub-title">Recent Use of Deep Learning for Fall Risk Assessment in Elderly and Parkinson’s Disease Patients: A Systematic Review</span><br/>
                <span class="pub-authors">Dr. Jayeeta Chakraborty, Harshit Agarwal</span><br/>
                <span class="pub-doi">Coming soon</span>
              </div>
              <div class="pub-header-toggle">
                <button class="pub-toggle" onclick="togglePubDesc('desc1')">Show Description ▼</button>
              </div>
            </div>
            <div class="pub-desc" id="desc1" style="display:none;">
              <p>Gait imbalances are a common cause of injury in elderly people, which can lead to instability and increase the risk of falling. The recent introduction of deep learning (DL)-based gait assessment methods has affected fall prevention systems in elderly and Parkinson's Disease (PD) patients. In this scoping review, we curate and provide a summary of these studies. We discuss the effectiveness, feasibility, and necessity of DL-based methods for predicting sudden imbalances in gait, including muscle strain, Freezing of Gait (FoG), etc. We also discuss the challenges and limitations in translating DL models to both clinical and daily activities, including variations in movement patterns, sensor availability, and the need for robust generalization. We highlight the limitations of the existing studies and future trends that need to be addressed by the research community.</p>
            </div>
          </div>
          <div class="pub-item">
            <div class="pub-header" style="display:flex; align-items:flex-start; justify-content:space-between; gap:1.5em;">
              <div class="pub-header-main">
                <span class="pub-title">GaitSetPy: A Professional Toolkit for Gait Analysis</span><br/>
                <span class="pub-authors">Harshit Agarwal, Dr. Jayeeta Chakraborty</span><br/>
                <span class="pub-doi">DOI: <a href="https://doi.org/10.5281/zenodo.16029953" target="_blank">10.5281/zenodo.16029953</a></span>
              </div>
              <div class="pub-header-toggle">
                <button class="pub-toggle" onclick="togglePubDesc('desc2')">Show Description ▼</button>
              </div>
            </div>
            <div class="pub-desc" id="desc2" style="display:none;">
              <p>GaitSetPy is a modular, extensible Python toolkit designed to advance research and development in gait analysis and recognition. Addressing the growing need for reproducible, flexible, and scalable software in human movement studies, GaitSetPy provides a unified framework for loading diverse gait datasets, preprocessing time series data, extracting a wide range of gait features, performing exploratory data analysis, and applying state-of-the-art machine learning models. The architecture is built around abstract base classes and singleton managers, enabling seamless integration of new datasets, feature extractors, preprocessors, analyzers, and classifiers through a plugin-based system. GaitSetPy supports both modern class-based and legacy function-based APIs, ensuring backward compatibility while promoting best practices in software design. The toolkit includes ready-to-use workflows for popular datasets such as <a href="https://archive.ics.uci.edu/ml/datasets/Daphnet+Freezing+of+Gait" target="_blank">Daphnet</a>, MobiFall, <a href="https://physionet.org/content/gaitdb/1.0.0/" target="_blank">PhysioNet</a>, <a href="https://sites.google.com/up.edu.mx/har-up/" target="_blank">HAR-UP</a>, and Arduous, and offers utilities for statistical, time, and frequency domain feature extraction, as well as visualization and evaluation tools. By fostering modularity, extensibility, and ease of use, GaitSetPy aims to accelerate innovation and reproducibility in gait research. The package is open source and available at <a href="https://github.com/Alohomora-Labs/gaitSetPy" target="_blank">https://github.com/Alohomora-Labs/gaitSetPy</a></p>
            </div>
          </div>
          <!-- <div class="pub-item">
            <div class="pub-header" style="display:flex; align-items:flex-start; justify-content:space-between; gap:1.5em;">
              <div class="pub-header-main">
                <span class="pub-title">Benchmarking Preprocessing, Feature Extraction, and Machine Learning Methods for Emotional Detection Using EEG Data</span><br/>
                <span class="pub-authors">Harshit Agarwal, Dr. Jayeeta Chakraborty</span><br/>
                <span class="pub-doi">Coming soon</span>
              </div>
              <div class="pub-header-toggle">
                <button class="pub-toggle" onclick="togglePubDesc('desc3')">Show Description ▼</button>
              </div>
            </div>
            <div class="pub-desc" id="desc3" style="display:none;">
              <p>Emotional detection from EEG signals presents unique challenges due to the complex, non-stationary nature of brain signals and the subjective interpretation of emotions. This comprehensive benchmarking study evaluates the effectiveness of various preprocessing techniques, feature extraction methods, and machine learning algorithms for emotion classification using EEG data. We systematically compare traditional signal processing approaches (bandpass filtering, artifact removal, spectral analysis) with advanced preprocessing methods (independent component analysis, wavelet transforms, adaptive filtering). For feature extraction, we benchmark time-domain features (statistical moments, Hjorth parameters), frequency-domain features (power spectral density, band power ratios), and time-frequency features (wavelet coefficients, spectrograms) against emerging deep learning-based feature representations. Our evaluation encompasses classical machine learning algorithms (SVM, Random Forest, k-NN), ensemble methods, and state-of-the-art deep learning architectures (CNNs, RNNs, Transformers) across multiple publicly available EEG emotion datasets. We provide detailed performance metrics, computational complexity analysis, and practical recommendations for researchers and practitioners in affective computing and brain-computer interfaces. The study also addresses critical challenges such as inter-subject variability, cross-dataset generalization, and real-time processing constraints, offering insights into the optimal pipeline for emotion detection applications.</p>
            </div>
          </div> -->
        </div>
      </div>
    </section>
    <section class="join-us">
      <div class="container">
        <h2 class="join-title">Join Us</h2>
        <div class="join-flex">
          <div class="join-guide">
            <h3>What We Look For</h3>
            <ul>
              <li>Strong foundation in machine learning and deep learning</li>
              <li>Interest in motion analysis, gait research, or biotechnology</li>
              <li>Experience with Python and open-source collaboration</li>
              <li>Curiosity, creativity, and a collaborative spirit</li>
            </ul>
          </div>
          <div class="join-methods">
            <h3>How to Apply</h3>
            <ol>
              <li><strong>Open Source:</strong> Fork our <a href="https://github.com/alohomora-labs" target="_blank">software repositories</a> and contribute directly—no need to be a formal lab member!</li>
              <li><strong>Email:</strong> Send your resume/CV and/or a statement of purpose to <a href="mailto:drjayc@alohomora-labs.me">drjayc@alohomora-labs.me</a>. We’ll review your application and schedule an interview if there’s a fit.</li>
            </ol>
          </div>
        </div>
      </div>
    </section>
  </main>
  <footer>
    <div class="container">
      <span>&copy; 2025 Alohomora Labs. All rights reserved.</span>
    </div>
  </footer>
  <script>
    function toggleMenu() {
      const hamburger = document.querySelector('.hamburger');
      const nav = document.getElementById('nav-menu');
      hamburger.classList.toggle('active');
      nav.classList.toggle('active');
    }
    
    function handleNavClick(event) {
      // Only handle mobile nav clicks
      if (window.innerWidth <= 800) {
        event.preventDefault();
        const clickedLink = event.target;
        const targetUrl = clickedLink.href;
        
        // Add fill animation to the clicked button
        clickedLink.classList.add('fill-animation');
        
        // Wait for animation to complete, then navigate
        setTimeout(function() {
          window.location.href = targetUrl;
        }, 800); // Match the buttonFill animation duration
      }
    }
    
    // Add click handlers to all nav links
    document.addEventListener('DOMContentLoaded', function() {
      const navLinks = document.querySelectorAll('#nav-menu a');
      navLinks.forEach(link => {
        link.addEventListener('click', handleNavClick);
      });
    });
    
    function togglePubDesc(id) {
      var el = document.getElementById(id);
      var btn = event.target;
      
      if (el.style.display === 'none' || el.style.display === '') {
        // Opening
        el.style.display = 'block';
        el.classList.remove('closing');
        btn.textContent = 'Hide Description ▲';
      } else {
        // Closing
        el.classList.add('closing');
        btn.textContent = 'Show Description ▼';
        
        // Wait for animation to complete before hiding
        setTimeout(function() {
          el.style.display = 'none';
          el.classList.remove('closing');
        }, 300); // Match the slideUp animation duration
      }
    }
  </script>
</body>
</html>
